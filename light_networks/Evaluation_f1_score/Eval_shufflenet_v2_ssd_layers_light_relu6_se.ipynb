{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "sys.path.append(os.path.abspath('../../extra_files'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from light_models.keras_ssd300_shufflenetv2_ssdlayers_light_relu6_se import ssd_300\n",
    "\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from extra_files import helper\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameteres needed for ssd_300() and SSDInputEncoder()\n",
    "\n",
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [1., 1., 1.] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "divide_by_stddev = [127.5, 127.5, 127.5]\n",
    "swap_channels = False # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 1 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [16, 30, 60, 100, 150, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True\n",
    "confidence_thresh=0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Build the Keras model.\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='inference',\n",
    "                scale_factor=1.5,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                divide_by_stddev=divide_by_stddev,\n",
    "                swap_channels=swap_channels,\n",
    "               confidence_thresh=confidence_thresh)\n",
    "\n",
    "# 2: Load some weights into the model.\n",
    "model.load_weights('/home/aldo/Documents/weights/light_models/PASCAL/shufflenet_v2_ssdlayers_light_relu6_se_factor_1.5.h5', by_name=True)\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3444788"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# factor 1.5\n",
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aldo/anaconda3/envs/test2/lib/python3.6/site-packages/ipykernel_launcher.py:31: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    }
   ],
   "source": [
    "from scipy import misc\n",
    "file_label = pd.read_csv('/home/aldo/Documents/data-cic/preprocess_data/PASCAL_test.csv')\n",
    "# get all images' names\n",
    "file_column = file_label.columns\n",
    "img_val = file_label[file_column[0]].unique()\n",
    "\n",
    "normalized_label = []\n",
    "predictions = np.zeros(shape=(1, 200, 6))\n",
    "\n",
    "# Iterate over images\n",
    "for start_i in range(0, len(img_val), 32):\n",
    "    end_i = start_i + 32\n",
    "    input_ = []\n",
    "    for img_name in img_val[start_i:end_i]:\n",
    "        img = imread('/home/aldo/Documents/data-cic/PASCAL' + '/' + img_name)\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "\n",
    "        # get labels from image\n",
    "        original_label = file_label[file_label[file_column[0]] == img_name].values[:, 1:-1]\n",
    "\n",
    "        # change formato from xmin, xmax, ymin, ymax to x, y, width, height\n",
    "        new_label = []\n",
    "        for o_label in original_label:\n",
    "            new_label.append([o_label[0], o_label[2], o_label[1] - o_label[0], o_label[3]- o_label[2]])\n",
    "            \n",
    "        new_label = helper.normilize_boxes(new_label, width, height)\n",
    "        normalized_label.append(new_label)\n",
    "\n",
    "        # resize image\n",
    "        resized_img= misc.imresize(img, size=(300, 300))\n",
    "        input_.append(resized_img)\n",
    "    input_ = np.array(input_)\n",
    "    input_ = input_.reshape(-1, 300, 300, 3)\n",
    "    pred = model.predict(input_)\n",
    "    predictions = np.append(predictions, pred, axis=0)\n",
    "\n",
    "predictions = predictions[1:] # delete empty item\n",
    " \n",
    "# Remove class and confidence from predictions\n",
    "predictions = helper.clean_predictions(predictions, id_class=1)\n",
    "predictions = helper.adjust_predictions(predictions)\n",
    "predictions = helper.get_coordinates(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 2097\n",
      "Presicion: 0.628\n",
      "Recall: 0.5379\n",
      "F1 score: 0.5794\n"
     ]
    }
   ],
   "source": [
    "# factor 1.5\n",
    "presicion, recall, f1_score = helper.cal_performance(normalized_label, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 2097\n",
      "Presicion: 0.7407\n",
      "Recall: 0.6331\n",
      "F1 score: 0.6827\n"
     ]
    }
   ],
   "source": [
    "# factor 1.5 threshold 0.5\n",
    "presicion, recall, f1_score = helper.cal_performance(normalized_label, predictions, iou=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test2]",
   "language": "python",
   "name": "conda-env-test2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
