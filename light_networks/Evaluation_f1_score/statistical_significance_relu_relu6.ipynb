{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "sys.path.append(os.path.abspath('../../extra_files'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from light_models.keras_ssd300_shufflenetv2_ssdlayers_no_shuffle_light_relu6 import ssd_300\n",
    "# from light_models.keras_ssd300_shufflenetv2_ssdlayers_no_shuffle_light import ssd_300\n",
    "\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from extra_files import helper\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameteres needed for ssd_300() and SSDInputEncoder()\n",
    "\n",
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [1., 1., 1.] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "divide_by_stddev = [127.5, 127.5, 127.5]\n",
    "swap_channels = False # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 1 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [16, 30, 60, 100, 150, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True\n",
    "confidence_thresh=0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Build the Keras model.\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='inference',\n",
    "                scale_factor=1.5,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                divide_by_stddev=divide_by_stddev,\n",
    "                swap_channels=swap_channels,\n",
    "               confidence_thresh=confidence_thresh)\n",
    "\n",
    "# 2: Load some weights into the model.\n",
    "model.load_weights('/home/aldo/Documents/weights/light_models/PASCAL/shufflenet_v2_ssdlayers_no_shuffle_light_relu6_factor_1.5.h5', by_name=True)\n",
    "# model.load_weights('/home/aldo/Documents/weights/light_models/PASCAL/shufflenet_v2_ssdlayers_no_shuffle_light_factor_1.5.h5', by_name=True)\n",
    "\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_errors(predictions, ground_t):\n",
    "    '''\n",
    "    '''\n",
    "    detections = []\n",
    "    \n",
    "    for index_pred in range(len(predictions)):\n",
    "        # Iter each bounding box\n",
    "        for item_to_eval in predictions[index_pred]:\n",
    "            best_iou = helper.best_match(item_to_eval, ground_t[index_pred])\n",
    "            if best_iou > 0.5:\n",
    "                detections.append(1)\n",
    "            else:\n",
    "                detections.append(0)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aldo/anaconda3/envs/tensor/lib/python3.6/site-packages/ipykernel_launcher.py:31: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    }
   ],
   "source": [
    "from scipy import misc\n",
    "file_label = pd.read_csv('/home/aldo/Documents/data-cic/preprocess_data/PASCAL_test.csv')\n",
    "# get all images' names\n",
    "file_column = file_label.columns\n",
    "img_val = file_label[file_column[0]].unique()\n",
    "\n",
    "normalized_label = []\n",
    "predictions = np.zeros(shape=(1, 200, 6))\n",
    "\n",
    "# Iterate over images\n",
    "for start_i in range(0, len(img_val), 32):\n",
    "    end_i = start_i + 32\n",
    "    input_ = []\n",
    "    for img_name in img_val[start_i:end_i]:\n",
    "        img = imread('/home/aldo/Documents/data-cic/PASCAL' + '/' + img_name)\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "\n",
    "        # get labels from image\n",
    "        original_label = file_label[file_label[file_column[0]] == img_name].values[:, 1:-1]\n",
    "\n",
    "        # change formato from xmin, xmax, ymin, ymax to x, y, width, height\n",
    "        new_label = []\n",
    "        for o_label in original_label:\n",
    "            new_label.append([o_label[0], o_label[2], o_label[1] - o_label[0], o_label[3]- o_label[2]])\n",
    "\n",
    "        new_label = helper.normilize_boxes(new_label, width, height)\n",
    "        normalized_label.append(new_label)\n",
    "\n",
    "        # resize image\n",
    "        resized_img= misc.imresize(img, size=(300, 300))\n",
    "        input_.append(resized_img)\n",
    "    input_ = np.array(input_)\n",
    "    input_ = input_.reshape(-1, 300, 300, 3)\n",
    "    pred = model.predict(input_)\n",
    "    predictions = np.append(predictions, pred, axis=0)\n",
    "\n",
    "predictions = predictions[1:] # delete empty item\n",
    "\n",
    "# Remove class and confidence from predictions\n",
    "predictions = helper.clean_predictions(predictions, id_class=1)\n",
    "predictions = helper.adjust_predictions(predictions)\n",
    "predictions = helper.get_coordinates(predictions)\n",
    "\n",
    "detections_relu6 = diagnostic_errors(normalized_label, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imread('/home/aldo/Documents/data-cic/PASCAL' + '/' + img_val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = helper.normilize_to_pixel(predictions[1:2], img.shape[1], img.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper.show_image_bb(img, pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5227"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5227"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(detections_relu6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3179\n"
     ]
    }
   ],
   "source": [
    "detections_relu6[50:75]\n",
    "print(len([x for x in detections_relu6 if x == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3146\n"
     ]
    }
   ],
   "source": [
    "detections[50:75]\n",
    "print(len([x for x in detections if x == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_yes = 0\n",
    "no_no = 0\n",
    "yes_no = 0\n",
    "no_yes = 0\n",
    "\n",
    "for index in range(len(detections)):\n",
    "    if detections[index] == 1 and detections_relu6[index] == 1:\n",
    "        yes_yes += 1\n",
    "    elif detections[index] == 0 and detections_relu6[index] == 0:\n",
    "        no_no += 1\n",
    "    elif detections[index] == 1 and detections_relu6[index] == 0:\n",
    "        yes_no += 1\n",
    "    elif detections[index] == 0 and detections_relu6[index] == 1:\n",
    "        no_yes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5227"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_yes + no_no + yes_no + no_yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2863\n",
      "1765\n",
      "283\n",
      "316\n"
     ]
    }
   ],
   "source": [
    "print(yes_yes)\n",
    "print(no_no)\n",
    "print(yes_no)\n",
    "print(no_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail to reject H0\n"
     ]
    }
   ],
   "source": [
    "statistics = ((yes_no - no_yes)**2) / (yes_no + no_yes)\n",
    "\n",
    "if statistics > alpha:\n",
    "    print('fail to reject H0')\n",
    "else:\n",
    "    print('reject H0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8180300500834725"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_yes = 2863\n",
    "no_no = 1765\n",
    "yes_no = 283\n",
    "no_yes = 316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail to reject H0\n"
     ]
    }
   ],
   "source": [
    "statistics = ((abs(yes_no - no_yes) - 1)**2) / (yes_no + no_yes)\n",
    "\n",
    "if statistics > alpha:\n",
    "    print('fail to reject H0')\n",
    "else:\n",
    "    print('reject H0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7095158597662772"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensor]",
   "language": "python",
   "name": "conda-env-tensor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
